{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ActividadIntegradora2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs1_ilJP1C71"
      },
      "source": [
        "import nltk\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qar97h1SUjWI"
      },
      "source": [
        "Tutorial 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2H86rFW1JAz"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTsD9eQ-DNi"
      },
      "source": [
        "# tokenizing - word tokenizers ... sentence tokenizers . Separate by words ... separate by sentences.\n",
        "# lexicon and corporas. \n",
        "# corporas - body of text all around the same thing.\n",
        "# lexicon - words and their meanings."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4G4AIt__Cen"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "example_text = \"Hello Mr. Smith, how are you doing today? The wheater is great and Python is awesome. The sky is pinish-blue. You should not eat cardboard.\"\n",
        "\n",
        "\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))\n",
        "\n",
        "for i in word_tokenize(example_text):\n",
        "  print(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDqIGPSuUyZk"
      },
      "source": [
        "Tutorial 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdGymYRxU0wt"
      },
      "source": [
        "#Stop words. Palabras que no importan, que no tienen significado. Para data analysis no importan.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sentence = \"This is an example showing off stop world filtration.\"\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "#print(stop_words)\n",
        "\n",
        "words = word_tokenize(example_sentence)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "  \n",
        "print(filtered_sentence)\n",
        "\n",
        "#fileterd_sentence = [w for w in fords if not w in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNV4271DihUh"
      },
      "source": [
        "Tutorial 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyhi4DpmiiqO"
      },
      "source": [
        "#Steming - A form of data pre processing, take de root stem of the word\n",
        "# Esto se hace porque muchas veces se tienen diferentes variaciones de las palabras pero significan lo mismo. \n",
        "#Porterstemer Algoritmo para hacer stem. \n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "example_words = [\"phyton\",\"phytoner\",\"phytoning\",\"phytoned\",\"phytonly\"]\n",
        "\n",
        "#for w in example_words:\n",
        "# print(ps.stem(w))\n",
        "\n",
        "new_text = \"It is very important to be pythonly while you are pythoning with phyton. All phytoners have pythoned poorly at least once.\"\n",
        "\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkyl1VUak9hk"
      },
      "source": [
        "Tutorial 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsJWaH2BuPts"
      },
      "source": [
        "#Part of speech taging. Pre processing. Labeling the part of speech in every single word. \n",
        "\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "process_content()  #Creates touple, of the word and then the part of speech."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
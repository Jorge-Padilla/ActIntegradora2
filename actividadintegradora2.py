# -*- coding: utf-8 -*-
"""ActividadIntegradora2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GB5i7TnFBCzdWlAfaOg_59xP0vl1kvzX
"""

import nltk
import sklearn

"""Tutorial 1"""

nltk.download()

# tokenizing - word tokenizers ... sentence tokenizers . Separate by words ... separate by sentences.
# lexicon and corporas. 
# corporas - body of text all around the same thing.
# lexicon - words and their meanings.

from nltk.tokenize import sent_tokenize, word_tokenize

example_text = "Hello Mr. Smith, how are you doing today? The wheater is great and Python is awesome. The sky is pinish-blue. You should not eat cardboard."


print(sent_tokenize(example_text))
print(word_tokenize(example_text))

for i in word_tokenize(example_text):
  print(i)

"""Tutorial 2"""

#Stop words. Palabras que no importan, que no tienen significado. Para data analysis no importan.

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

example_sentence = "This is an example showing off stop world filtration."
stop_words = set(stopwords.words("english"))
#print(stop_words)

words = word_tokenize(example_sentence)

filtered_sentence = []

for w in words:
  if w not in stop_words:
    filtered_sentence.append(w)
  
print(filtered_sentence)

#fileterd_sentence = [w for w in fords if not w in stop_words]

"""Tutorial 3"""

#Steming - A form of data pre processing, take de root stem of the word
# Esto se hace porque muchas veces se tienen diferentes variaciones de las palabras pero significan lo mismo. 
#Porterstemer Algoritmo para hacer stem. 

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()

example_words = ["phyton","phytoner","phytoning","phytoned","phytonly"]

#for w in example_words:
# print(ps.stem(w))

new_text = "It is very important to be pythonly while you are pythoning with phyton. All phytoners have pythoned poorly at least once."

words = word_tokenize(new_text)

for w in words:
  print(ps.stem(w))

"""Tutorial 4"""

#Part of speech taging. Pre processing. Labeling the part of speech in every single word. 

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
  try:
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      print(tagged)
  except Exception as e:
    print(str(e))

process_content()  #Creates touple, of the word and then the part of speech.
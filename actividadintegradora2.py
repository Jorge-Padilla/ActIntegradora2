# -*- coding: utf-8 -*-
"""ActividadIntegradora2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GB5i7TnFBCzdWlAfaOg_59xP0vl1kvzX
"""

# Commented out IPython magic to ensure Python compatibility.
import nltk
import sklearn
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('Agg')
import numpy as np
# if using a Jupyter notebook, include:
# %matplotlib inline

"""Tutorial 1"""

nltk.download()

# Poner la d de download, despues escribir all, esperar y luego poner q de quit.

# tokenizing - word tokenizers ... sentence tokenizers . Separate by words ... separate by sentences.
# lexicon and corporas. 
# corporas - body of text all around the same thing.
# lexicon - words and their meanings.

from nltk.tokenize import sent_tokenize, word_tokenize

example_text = "Hello Mr. Smith, how are you doing today? The wheater is great and Python is awesome. The sky is pinish-blue. You should not eat cardboard."


print(sent_tokenize(example_text))
print(word_tokenize(example_text))

for i in word_tokenize(example_text):
  print(i)

"""Tutorial 2"""

#Stop words. Palabras que no importan, que no tienen significado. Para data analysis no importan.

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

example_sentence = "This is an example showing off stop world filtration."
stop_words = set(stopwords.words("english"))
#print(stop_words)

words = word_tokenize(example_sentence)

filtered_sentence = []

for w in words:
  if w not in stop_words:
    filtered_sentence.append(w)
  
print(filtered_sentence)

#fileterd_sentence = [w for w in fords if not w in stop_words]

"""Tutorial 3"""

#Steming - A form of data pre processing, take de root stem of the word
# Esto se hace porque muchas veces se tienen diferentes variaciones de las palabras pero significan lo mismo. 
#Porterstemer Algoritmo para hacer stem. 

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()

example_words = ["phyton","phytoner","phytoning","phytoned","phytonly"]

#for w in example_words:
# print(ps.stem(w))

new_text = "It is very important to be pythonly while you are pythoning with phyton. All phytoners have pythoned poorly at least once."

words = word_tokenize(new_text)

for w in words:
  print(ps.stem(w))

"""Tutorial 4"""

#Part of speech taging. Pre processing. Labeling the part of speech in every single word. 

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
  try:
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      print(tagged)
  except Exception as e:
    print(str(e))

process_content()  #Creates touple, of the word and then the part of speech.

"""Tutorial 5"""

#Chunking. 

#Part of speech taging. Pre processing. Labeling the part of speech in every single word. 

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
  try:
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      chunkGram = """Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}"""  #RB is Any form of an adverb, we are looking for zero or more of these. VB is verb and NNP is proper noun. NN is a nown.
      
      chunkParser = nltk.RegexpParser(chunkGram)
      chunked = chunkParser.parse(tagged)

      print(chunked)


  except Exception as e:
    print(str(e))

process_content()  #Creates touple, of the word and then the part of speech.

"""Tutorial 6"""

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
  try:
    for i in tokenized[5:]:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      chunkGram = """Chunk: {<.*>+}
                              }<VB.?|IN|DT|TO>+{"""  # Chunk everything except for verb, prepositions or determinators. 
      
      chunkParser = nltk.RegexpParser(chunkGram)
      chunked = chunkParser.parse(tagged)

      print(chunked)

  except Exception as e:
    print(str(e))

process_content()  #Creates touple, of the word and then the part of speech.

"""Tutorial 7"""

#named entity recognition.
from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer #Unsupervised ML sentence tokenizer. Ya esta pre entrenado

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
  try:
    for i in tokenized[5:]:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      namedEnt = nltk.ne_chunk(tagged, binary = True)
      print(namedEnt)

  except Exception as e:
    print(str(e))

process_content()  #Creates touple, of the word and then the part of speech.

"""Tutorial 8"""

# Lematizing Es una operacion similar a steamming, pero el resultado es una palabra, podr√≠a ser un sinonimo o la palabra original. 
# Es un poco mejor que stemming.
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize("cats"))
print(lemmatizer.lemmatize("cacti"))
print(lemmatizer.lemmatize("geese"))
print(lemmatizer.lemmatize("rocks"))
print(lemmatizer.lemmatize("phyton"))

print(lemmatizer.lemmatize("better",pos="a")) # A de adjetivo
print(lemmatizer.lemmatize("better")) # El valor default es pos = n que es de noun.
print(lemmatizer.lemmatize("best",pos="a"))
print(lemmatizer.lemmatize("run"))
print(lemmatizer.lemmatize("run",pos="v")) # V de verbo

"""Tutorial 9"""

# Tutorial 9